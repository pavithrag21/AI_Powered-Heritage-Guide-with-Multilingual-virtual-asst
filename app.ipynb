{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4960b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import all the necessary packages \n",
    "# install -r requirements.txt\n",
    "import whisper\n",
    "from gtts import gTTS\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import tempfile\n",
    "from IPython.display import Audio\n",
    "from deep_translator import GoogleTranslator \n",
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c164e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/huggingface/transformers\n",
    "%pip install deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a597125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datasetimport pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"Cleaned_Temples_Significance.csv\")\n",
    "\n",
    "# Folder where images are stored\n",
    "image_folder = \"images\"\n",
    "\n",
    "# Load images using PIL and add to dataframe\n",
    "def load_image(row):\n",
    "    image_path = os.path.join(image_folder, row['image_url'])\n",
    "    return Image.open(image_path)\n",
    "\n",
    "# Apply the function to create a new column with PIL images\n",
    "df['image_pil'] = df.apply(load_image, axis=1)\n",
    "\n",
    "# Convert dataset to dictionary\n",
    "temple_info = {row[\"name\"]: row[\"image_url\"] for _, row in df.iterrows()}\n",
    "\n",
    "print(temple_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfc7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# # Load Temple Dataset\n",
    "# df = pd.read_csv(\"Dataset1.csv\")  # must contain 'name', 'description'\n",
    "\n",
    "# # Encode all descriptions\n",
    "# embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# df['description_embedding'] = df['description'].apply(lambda desc: embedder.encode(desc, convert_to_tensor=True))\n",
    "\n",
    "# def get_caption(image_path):\n",
    "#     raw_image = Image.open(image_path).convert('RGB')\n",
    "#     inputs = blip_processor(raw_image, return_tensors=\"pt\").to(blip_model .device)\n",
    "#     out = blip_model .generate(**inputs)\n",
    "#     return blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# def identify_temple_with_blip(image_path):\n",
    "#     caption = get_caption(image_path)\n",
    "#     caption_embed = embedder.encode(caption, convert_to_tensor=True)\n",
    "\n",
    "#     # Semantic similarity\n",
    "#     similarities = [util.pytorch_cos_sim(caption_embed, emb)[0][0].item() for emb in df['description_embedding']]\n",
    "#     best_index = similarities.index(max(similarities))\n",
    "    \n",
    "#     return {\n",
    "#         \"caption\": caption,\n",
    "#         \"name\": df.iloc[best_index]['name'],\n",
    "#         \"description\": df.iloc[best_index]['description']\n",
    "#     }\n",
    "\n",
    "# # üîç Example\n",
    "# result = identify_temple_with_blip(\"D:\\Heritage_chatbot\\images\\image6.jfif\")\n",
    "# print(\"üèØ Temple Name:\", result['name'])\n",
    "# print(\"üìñ Description:\", result['description'])\n",
    "# print(\"üñºÔ∏è Caption:\", result['caption'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08200e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# #Step 2: Load your temple dataset ===\n",
    "# df = pd.read_csv(\"Cleaned_Temples_Significance.csv\")  # contains columns: name, description, image_url\n",
    "# image_folder = \"images\"\n",
    "\n",
    "# def extract_features(pil_img):\n",
    "#     try:\n",
    "#         inputs = blip_processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = blip_model.vision_model(**inputs)\n",
    "#             features = outputs.pooler_output.squeeze(0)\n",
    "#         return features\n",
    "#     except Exception as e:\n",
    "#         print(f\"Feature extraction failed: {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# feature_vectors = []\n",
    "# names = []\n",
    "# descriptions = []\n",
    "\n",
    "# for _, row in df.iterrows():\n",
    "#     image_path = os.path.join(image_folder, row['image_url'])\n",
    "#     try:\n",
    "#         pil_img = Image.open(image_path).convert(\"RGB\")\n",
    "#         features = extract_features(pil_img)\n",
    "#         if features is not None:\n",
    "#             feature_vectors.append(features.cpu().numpy())\n",
    "#             names.append(row['name'])\n",
    "#             descriptions.append(row['description'])\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing image {image_path}: {e}\")\n",
    "\n",
    "# feature_matrix = np.array(feature_vectors)    \n",
    "\n",
    "# # === Step 4: Function to match uploaded image ===\n",
    "# def identify_temple(pil_img):\n",
    "#     input_feature = extract_features(pil_img)\n",
    "#     if input_feature is None:\n",
    "#         return \"Unknown Temple\", \"Feature extraction failed.\"\n",
    "\n",
    "#     input_vector = input_feature.unsqueeze(0).cpu().numpy()\n",
    "\n",
    "#     if feature_matrix.ndim == 1:\n",
    "#         feature_matrix = feature_matrix.reshape(1, -1)\n",
    "\n",
    "#     if feature_matrix.size == 0:\n",
    "#         return \"Unknown\", \"No reference data available.\"\n",
    "\n",
    "#     similarities = cosine_similarity(input_vector, feature_matrix)\n",
    "#     best_match_idx = similarities.argmax()\n",
    "\n",
    "#     return names[best_match_idx], descriptions[best_match_idx]\n",
    "# # === Step 5: Example usage ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     # uploaded_image_path = \"d:\\Heritage_chatbot\\sample_image.jfif\"  # Replace with your test image\n",
    "#     img = Image.open(\"D:/Heritage_chatbot/sample_image.jfif\").convert(\"RGB\")\n",
    "#     name, desc = identify_temple(img)\n",
    "#     print(f\"\\nüèõÔ∏è Temple Identified: {name}\\nüìñ Description: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ece09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image):\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\", max_length=100).to(device)\n",
    "    out = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "    # print(caption)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78534f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transcribe(audio):\n",
    "    \n",
    "    #time.sleep(3)\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    audio = whisper.load_audio(audio)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    # make log-Mel spectrogram and move to the same device as the model\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
    "\n",
    "    # detect the spoken language\n",
    "    _, probs = whisper_model.detect_language(mel)\n",
    "    print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "    # decode the audio\n",
    "    options = whisper.DecodingOptions(fp16 = False)\n",
    "    result = whisper.decode(whisper_model, mel, options)\n",
    "    return result.text\n",
    "    print(result.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install webdriver-manager\n",
    "%pip install selenium beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e27675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# ‚úÖ Your list of websites and the class names to extract\n",
    "sites_to_scrape = [\n",
    "    {\n",
    "        \"url\": \"https://www.holidify.com/collections/historical-places-in-tamilnadu\",\n",
    "        \"class_names\": [\n",
    "            \"card-text\",\n",
    "            \"card-heading\",\n",
    "            \"travel-essentials-section mt-md-0\",\n",
    "            \"slick-track\",\n",
    "            \"textColor infoSpace\",\n",
    "            \"card-body\",\n",
    "            \"places-covered\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.southtourism.in/tamil-nadu-monuments-timings.php\",\n",
    "        \"class_names\": [\n",
    "            \"card-body border\",\n",
    "            \"table table-bordered\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.tamilnadutourism.tn.gov.in/\",\n",
    "        \"class_names\": [\n",
    "            \"text-justify pb-0\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# ‚úÖ Set up headless Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# ‚úÖ Store LangChain Documents\n",
    "documents = []\n",
    "\n",
    "# ‚úÖ Scraping loop\n",
    "for site in sites_to_scrape:\n",
    "    url = site[\"url\"]\n",
    "    class_names = site[\"class_names\"]\n",
    "\n",
    "    print(f\"\\nüåê Visiting: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    for class_name in class_names:\n",
    "        elements = soup.find_all(class_=class_name)\n",
    "        if elements:\n",
    "            print(f\"\\n‚úÖ Found content for class: '{class_name}'\")\n",
    "            for i, el in enumerate(elements):\n",
    "                text = el.get_text(separator=\"\\n\", strip=True)\n",
    "                if text:\n",
    "                    doc = Document(\n",
    "                        page_content=text,\n",
    "                        metadata={\n",
    "                            \"source\": url,\n",
    "                            \"class\": class_name,\n",
    "                            \"index\": i\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è No content found for class: '{class_name}'\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# ‚úÖ Done! You now have a list of LangChain Document objects\n",
    "print(f\"\\nüìÑ Total documents extracted: {len(documents)}\")\n",
    "print(\"\\nüîç Sample content from first document:\\n\")\n",
    "print(documents[0].page_content[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32646e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "cleaned_docs = [Document(page_content=clean_html(doc.page_content), metadata=doc.metadata) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(cleaned_docs)\n",
    "# documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b295496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# üîß Load your local model using pipeline\n",
    "flan_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=flan_model,\n",
    "    tokenizer=flan_tokenizer,\n",
    "    max_length=2400,\n",
    "    do_sample=False,  # deterministic output\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_llm = HuggingFacePipeline(pipeline=flan_pipeline)\n",
    "\n",
    "# Setup RAG chain with your retriever\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=flan_llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bb12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    return detect(text)\n",
    "\n",
    "\n",
    "def translate_to_english(text, src_lang):\n",
    "    return GoogleTranslator(source=src_lang, target='en').translate(text)\n",
    "\n",
    "def translate_from_english(text, target_lang):\n",
    "    return GoogleTranslator(source='en', target=target_lang).translate(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69297d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multilingual_response(user_input):\n",
    "    detected_lang = detect_language(user_input)\n",
    "    \n",
    "    # Step 1: Translate to English if needed\n",
    "    if detected_lang != 'en':\n",
    "        english_input = translate_to_english(user_input, detected_lang)\n",
    "    else:\n",
    "        english_input = user_input\n",
    "\n",
    "    response_en = rag_chain.invoke(english_input)\n",
    "    result=response_en[\"result\"]\n",
    "\n",
    "    # Step 3: Translate back to user's language\n",
    "    if detected_lang != \"en\":\n",
    "        final_response = translate_from_english(result, detected_lang)\n",
    "    else:\n",
    "        final_response = result\n",
    "\n",
    "    return final_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"tell me about chennai\"\n",
    "print(detect_language(text))\n",
    "print(generate_multilingual_response(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7421981",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function: Generate voice\n",
    "def text_to_speech(text):\n",
    "    tts = gTTS(text)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as fp:\n",
    "        tts.save(fp.name)\n",
    "        return fp.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa40e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first gardio\n",
    "\n",
    "import gradio as gr\n",
    "def process_input(mode, image, audio, text_input):\n",
    "    if mode == \"Image\":\n",
    "        caption = generate_caption(image)\n",
    "        question = \"Tell me about this temple.\"\n",
    "        full_prompt = f\"{caption}\\nQuestion: {question}\"\n",
    "        response = rag_chain.invoke({\"query\": full_prompt})\n",
    "        answer = response[\"result\"]\n",
    "\n",
    "    elif mode == \"Audio\":\n",
    "        full_prompt = transcribe(audio)\n",
    "        response = rag_chain.invoke({\"query\": full_prompt})\n",
    "        answer = response[\"result\"]\n",
    "        # answer = generate_multilingual_response(text_input)\n",
    "\n",
    "    elif mode == \"Text\":\n",
    "        answer = generate_multilingual_response(text_input)\n",
    "\n",
    "    else:\n",
    "        return \"Invalid mode selected.\", None\n",
    "\n",
    "    tts_path = text_to_speech(answer)\n",
    "    return answer, tts_path\n",
    "\n",
    "def toggle_inputs(mode):\n",
    "    return (\n",
    "        gr.update(visible=(mode == \"Image\")),\n",
    "        gr.update(visible=(mode == \"Audio\")),\n",
    "        gr.update(visible=(mode == \"Text\"))\n",
    "    )\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    gr.Markdown(\"# üõï AI Heritage Guide\")\n",
    "    gr.Markdown(\"Choose how you want to interact: Image, Audio, or Text\")\n",
    "\n",
    "    mode = gr.Dropdown(\n",
    "        [\"Image\", \"Audio\", \"Text\"],\n",
    "        label=\"Choose Input Mode\",\n",
    "        value=\"Image\"\n",
    "    )\n",
    "\n",
    "    image_input = gr.Image(label=\"Upload Temple Image\", visible=True)\n",
    "    audio_input = gr.Audio(label=\"Speak your question\", visible=False,type=\"filepath\")\n",
    "    text_input = gr.Textbox(label=\"Type your question\", visible=False)\n",
    "\n",
    "    output_text = gr.Textbox(label=\"AI Response\")\n",
    "    output_audio = gr.Audio(label=\"Spoken Output\")\n",
    "\n",
    "    mode.change(fn=toggle_inputs, inputs=[mode], outputs=[image_input, audio_input, text_input])\n",
    "\n",
    "    submit_btn = gr.Button(\"Ask\")\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn=process_input,\n",
    "        inputs=[mode, image_input, audio_input, text_input],\n",
    "        outputs=[output_text, output_audio]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
